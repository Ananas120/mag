{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for the Answer Generator Split model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports + model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.qa import AnswerGeneratorSplit\n",
    "from utils.text import TextEncoder\n",
    "from utils import plot, plot_multiple, plot_embedding, plot_confusion_matrix, set_display_options\n",
    "from datasets import get_dataset, prepare_dataset, train_test_split, test_dataset_time\n",
    "\n",
    "set_display_options()\n",
    "\n",
    "model_name = 'test_nq_qa_generator_split_5_mean_2'\n",
    "bert_base  = 'facebook/bart-large'\n",
    "\n",
    "print(\"Tensorflow version : {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lang'             : 'en',\n",
    "    'input_format'     : ['{question}', '{context}'],\n",
    "    'output_format'    : '{answer}',\n",
    "    'text_encoder'     : TextEncoder.from_transformers_pretrained(bert_base),\n",
    "    'max_input_length' : 512,\n",
    "    \n",
    "    'pretrained' : bert_base,\n",
    "    'encoder_subsampling_step'   : 5,\n",
    "    'encoder_subsampling_offset' : 1,\n",
    "    'subsample_after' : True,\n",
    "    'encoder_subsampling_mode'   : 'mean'\n",
    "}\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "model = AnswerGeneratorSplit(nom = model_name, ** config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.model.encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model instanciation + dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnswerGeneratorSplit(nom = model_name)\n",
    "\n",
    "lr = {'name' : 'WarmupScheduler', 'maxval' : 5e-5,'minval' : 1e-5, 'factor' : 512, 'warmup_steps' : 8192}\n",
    "lr = 5e-5\n",
    "\n",
    "model.compile(optimizer = 'adam', optimizer_config = {'lr' : lr}, metrics = ['TextAccuracy'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = 'squad' #if 'nq' not in model_name else 'nq'\n",
    "\n",
    "dataset = get_dataset(datasets, clean_text = True, skip_impossible = True, keep_only_first = True)\n",
    "train, valid = dataset['train'], dataset['valid']\n",
    "\n",
    "\n",
    "print(\"Dataset length :\\n  Training set : {}\\n  Validation set : {}\".format(\n",
    "    len(train), len(valid)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(axis = 'index', inplace = True)\n",
    "valid.dropna(axis = 'index', inplace = True)\n",
    "\n",
    "\n",
    "print(\"Dataset length :\\n  Training set : {}\\n  Validation set : {}\".format(\n",
    "    len(train), len(valid)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.array([model.encode_data(row)[1][1] for row in tqdm(train.to_dict('records'))])\n",
    "print(freqs)\n",
    "plot(freqs, plot_type = 'hist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(freqs > 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning = True\n",
    "\n",
    "if fine_tuning:\n",
    "    model.get_optimizer().learning_rate.assign(1e-5)\n",
    "\n",
    "epochs = 5 if fine_tuning else 1\n",
    "if datasets == 'squad':\n",
    "    batch_size = 8 if fine_tuning else 32\n",
    "else:\n",
    "    batch_size = 6 if fine_tuning else 32\n",
    "shuffle_size = 0 if model.epochs + epochs < 3 else batch_size * 32\n",
    "\n",
    "augment_prct = 0.\n",
    "nb_mask = 1\n",
    "min_mask_length = 1\n",
    "max_mask_length = 1\n",
    "\n",
    "in_batch_negatives = True\n",
    "\n",
    "max_input_length = 512\n",
    "max_output_length = 128\n",
    "\n",
    "print(\"Training samples   : {} - {} batches\".format(len(train), len(train) // batch_size))\n",
    "print(\"Validation samples : {} - {} batches\".format(len(valid), len(valid) // (batch_size * 2)))\n",
    "\n",
    "model.model.freeze(trainable = fine_tuning)\n",
    "\n",
    "hist = model.train(\n",
    "    train, validation_data = valid, \n",
    "    epochs = epochs, batch_size = batch_size, valid_batch_size = 2.,\n",
    "    shuffle_size = shuffle_size, max_input_length = max_input_length, max_output_length = max_output_length,\n",
    "    in_batch_negatives = in_batch_negatives,\n",
    "    \n",
    "    augment_prct = augment_prct, nb_mask = nb_mask, min_mask_length = min_mask_length, max_mask_length = max_mask_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()\n",
    "print(model.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['question'] = valid['question'].apply(lambda q: q + ' ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.get_dataset_config(batch_size = 5, is_validation = True, shuffle_size = 0)\n",
    "ds = prepare_dataset(valid.sample(10, random_state = 0), ** config, debug = True)\n",
    "\n",
    "for batch in ds:\n",
    "    model.predict_with_target(batch, n_pred = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, question, context = None, ** kwargs):\n",
    "    if not isinstance(question, list): question = [question]\n",
    "    if context is not None:\n",
    "        if not isinstance(context, list) or len(context) != len(question): context = [context]\n",
    "    if len(context) == 1 and len(question) > 1: context = context * len(question)\n",
    "        \n",
    "    data = question if context is None else []\n",
    "    if context is not None:\n",
    "        for i, q in enumerate(question):\n",
    "            if not isinstance(q, dict): q = {'question' : q}\n",
    "            if len(context) == len(question):\n",
    "                c = context[i] if isinstance(context[i], dict) else {'context' : context[i]}\n",
    "            else:\n",
    "                c = {'context' : context} if not isinstance(context, dict) else context\n",
    "            data.append({** q, ** c})\n",
    "        \n",
    "    answers = []\n",
    "    for row in data:\n",
    "        inputs = [tf.expand_dims(inp, axis = 0) for inp in self.get_input(row)]\n",
    "        \n",
    "        pred = self.infer(inputs, training = False)\n",
    "\n",
    "        answers.append(self.decode_text(pred[0], remove_tokens = True))\n",
    "    \n",
    "    return answers\n",
    "\n",
    "question = [\n",
    "    'How is the night vision of cat ?',\n",
    "    'How is the night vision of cat ?',\n",
    "    'What is the anoatomy of a cat ?',\n",
    "    'How many paws does a cat have ?',\n",
    "    'How many paws does a cat have ?',\n",
    "    'What is the origin of life ?'\n",
    "]\n",
    "context  = [\n",
    "    'The cat is similar in anatomy to the other felid species: it has a strong flexible body, \\\n",
    "quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well \\\n",
    "developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-\\\n",
    "specific body language. A predator that is most active at dawn and dusk (crepuscular), the cat is a solitary hunter but a social species. \\\n",
    "It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals.[7] It secretes and \\\n",
    "perceives pheromones.',\n",
    "    [p.strip() + '.' for p in 'The cat is similar in anatomy to the other felid species: it has a strong flexible body, \\\n",
    "quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well \\\n",
    "developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-\\\n",
    "specific body language. A predator that is most active at dawn and dusk (crepuscular), the cat is a solitary hunter but a social species. \\\n",
    "It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals.[7] It secretes and \\\n",
    "perceives pheromones.'.split('...') if len(p) > 0],\n",
    "    ['The cat is similar in anatomy to the other felid species: it has a strong flexible body, \\\n",
    "quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well \\\n",
    "developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-\\\n",
    "specific body language. A predator that is most active at dawn and dusk (crepuscular), the cat is a solitary hunter but a social species. \\\n",
    "It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals.[7] It secretes and \\\n",
    "perceives pheromones.', 'The answer to everything is 42'],\n",
    "    'A cat is an animal which has 4 paws and whiskers.',\n",
    "    'A cat is an animal which has 4 paws and whiskers. However, everyone knows that the answer to everything is 42 !',\n",
    "    'The answer to everything is 42.'\n",
    "]\n",
    "\n",
    "#question, context = question[0], context[0]\n",
    "\n",
    "if not isinstance(question, list): question = [question]\n",
    "if not isinstance(context, list): context = [context]\n",
    "\n",
    "answers = predict(model, question, context)\n",
    "\n",
    "for q, c, a in zip(question, context, answers):\n",
    "    print(\"Question : {}\\nContext : {}\\nAnswer : {}\\n\".format(q, c, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.text_encoder._id_to_symbol[2])\n",
    "print(model.text_encoder.eos_token_idx)\n",
    "print(model.text_encoder.decode([0, 3714, 2]))\n",
    "print(chr(model.text_encoder.byte_encoder_inv['/']))\n",
    "\n",
    "model.text_encoder.special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.get_dataset_config(batch_size = 16, is_validation = False, shuffle_size = 0)\n",
    "ds = prepare_dataset(valid, ** config, debug = True)\n",
    "\n",
    "test_dataset_time(ds, steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_train_objects.optimizers import WarmupScheduler\n",
    "\n",
    "lr = WarmupScheduler(maxval = 1e-3, minval = 1e-4, factor = 256, warmup_steps = 4096)\n",
    "lr.plot(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = model.get_optimizer().learning_rate\n",
    "lr.assign(5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = model.get_optimizer().learning_rate\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.text_encoder)\n",
    "print(model.get_input(\"Hello\", \"World !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(tf.sequence_mask([4, 3], maxlen = 4), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reshape(tf.range(9), [3, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import get_dataset, prepare_dataset, test_dataset_time\n",
    "from utils.text import TextEncoder\n",
    "\n",
    "valid = get_dataset('nq', modes = 'valid', include_document = True)['valid']\n",
    "\n",
    "text_encoder = TextEncoder.from_transformers_pretrained('facebook/bart-large')\n",
    "question_format = '{question}'\n",
    "context_format  = '{context}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(question, ** kwargs):\n",
    "    return text_encoder.format(question_format, question = question, ** kwargs)\n",
    "    \n",
    "def format_context(context, title = None, ** kwargs):\n",
    "    return text_encoder.format(context_format, context = context, title = title, ** kwargs)\n",
    "    \n",
    "def tf_format_question(data):\n",
    "    q_text = data if not isinstance(data, (dict, pd.Series)) else data.get('question', '')\n",
    "    \n",
    "    encoded_text, token_types = tf.py_function(\n",
    "        format_question, [q_text], Tout = [tf.int32, tf.int32]\n",
    "    )\n",
    "    encoded_text.set_shape([None])\n",
    "    \n",
    "    return encoded_text\n",
    "\n",
    "def tf_format_context(data):\n",
    "    if not isinstance(data, (dict, pd.Series)): data = {'context' : data}\n",
    "    \n",
    "    encoded_text, token_types = tf.py_function(\n",
    "        format_context, [data.get('context', ''), data.get('title', '')], Tout = [tf.int32, tf.int32]\n",
    "    )\n",
    "    encoded_text.set_shape([None])\n",
    "    \n",
    "    return encoded_text\n",
    "\n",
    "        \n",
    "def get_input(data):\n",
    "    q_tokens = tf_format_question(data)\n",
    "    \n",
    "    if isinstance(data['context'], list):\n",
    "        contexts = [tf_format_context(c, t) for t, c in zip(data['title'], data['context'])]\n",
    "        \n",
    "        outputs = (q_tokens, len(q_tokens))\n",
    "        for c in contexts: outputs += (c, len(c))\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    c_tokens = tf_format_context(data)\n",
    "    \n",
    "    return (q_tokens, len(q_tokens), c_tokens, len(c_tokens))\n",
    "\n",
    "def get_dataset_config(** kwargs):\n",
    "    kwargs.update({\n",
    "        'batch_before_map'  : True,\n",
    "        'padded_batch'      : True,\n",
    "        'pad_kwargs'        : {\n",
    "            'padded_shapes'     : (\n",
    "                ((None,), (), (None, None), (None, ))\n",
    "            ),\n",
    "            'padding_values'    : (\n",
    "                (text_encoder.blank_token_idx, 0, text_encoder.blank_token_idx, 0)\n",
    "            )\n",
    "        }\n",
    "    })\n",
    "        \n",
    "    return kwargs\n",
    "\n",
    "config = get_dataset_config(batch_size = 2)\n",
    "\n",
    "valid['context'] = valid['paragraphs']\n",
    "valid['title']   = valid['titles']\n",
    "\n",
    "dataset = prepare_dataset(valid, is_rectangular = False, encode_fn = get_input, ** config, debug = True)\n",
    "test_dataset_time(dataset, steps = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.data.Dataset.from_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
